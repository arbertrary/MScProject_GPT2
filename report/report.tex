\documentclass[sigconf,natbib=false]{acmart}

%%%%%%%%
% Packages
%%%%%%%%

\usepackage[backend=biber]{biblatex}

% Filter warnings issued by package biblatex starting with "Patching footnotes failed"
% Source: https://tex.stackexchange.com/questions/202988/beamer-patching-footnotes-warning-patching-footnotes-failed-footnote-detectio
\usepackage{silence}
\WarningFilter{biblatex}{Patching footnotes failed}

% For formal tables
\usepackage{booktabs} 
\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{csquotes}

%%%%%%%%
% Remove copyright
%%%%%%%%
\setcopyright{none}
\settopmatter{printacmref=false}
\acmISBN{} % set this to remove ISBN
\acmDOI{} % set this to remove DOI


%%%%%%%%
% Meta information
%%%%%%%%
\acmConference[]
	{Masterpraktikum Lehrstuhl X}
	{SoSe \the\year}


%%%%%%%%
% Bibliography sources
%%%%%%%%

% * you can use a remote bibliography from BibSonomy (change 'dmir' to your own username)
%\addbibresource[location=remote]{http://www.bibsonomy.org/bib/user/dmir/myown}

% * or a local file
\addbibresource{bibliography.bib}



\begin{document}

%%%%%%%%
% Front matter
%%%%%%%%

\title{Bericht Masterpraktikum}
%\subtitle{An optional subtitle}

\author{Armin Bernstetter}
\affiliation{%
  \institution{University of W\"urzburg}
  \today
}
% \email{trovato@corporation.com}


\begin{abstract}
This report is a summary of the 
\end{abstract}


%%%%%%%%
% Content
%%%%%%%%

\maketitle

\section{Introduction/Motivation}

The general task of this project was to expand a corpus of german text and then train language models on this corpus.

This was based on the success of OpenAI's GPT-2 project which trained a language model on 40gb of english web text. The model was so successful in tasks such as text generation that OpenAI refused to release the training code and the entire model out of ethical reasons. The concern was that malicious parties could use GPT-2 to generate e.g. fake news for their own good.

\begin{itemize}
	\item dataset erweitern
	\item language model
\end{itemize}


\section{Crawler}

Several available german text corpora were already included in the existing corpus. This included the german Wikipedia and novels. 

Crawling the web or rather specific pages for german text is a good way to get new text data.


\paragraph{Reddit}
Pages that were considered initially were german subreddits on Reddit. This includes subreddits such as \textbf{reddit.com/r/de} or \textbf{reddit.com/r/de\_iama}. Reddit's API unfortunately limits the amount of requests a script can make and would have required a workaround repeatedly requesting new tokens etc.

For future work it could make sense to look into \url{https://pushshift.io/} which offers a number of Reddit data dumps, mostly from some years ago.

\paragraph{Wattpad}

Another page that was considered briefly was Wattpad \url{https://www.wattpad.com/}, a \enquote{social storytelling} platform. Users can share their own stories there and read those of other users. Upon examination of the web page's structure it was decided that Wattpad are not keen on users crawling their site. Therefore Wattpad was dismissed as well.

\paragraph{Team Andro} The website Team Andro \url{https://www.team-andro.com/} is a german Bodybuilding and weightlifting page. It contains articles on training, nutrition, events and more. More importantly, however, it is accompanied by a large german web forum with a user count of almost 300.000 and around 10 million forum posts in total (last checked 2019-09-25).

Upon inquiry the site administrator provided some advice and a link to Team-Andro's sitemap which allowed to iterate through most of the forum's publicly available HTML pages.

These pages were then parsed from HTML and saved in a directory structure representing the forum structure of Team Andro with each post represented as one text file. Additionally, meta data such as user names, time stamps etc were saved in a mongoDB database.


\section{The Dataset}

Prior to the addition of the Team-Andro text data, the existing corpus consisted of ... TODO


The Team-Andro corpus consists of mostly german web text. As such it is often unstructured, meaningless text without contiguous sentences. 

\begin{itemize}
	\item der bestehende super corpus
	\item Der neue Team Andro Corpus
\end{itemize}

\section{Elmo}

\begin{itemize}
	\item Visualisierte elmo embeddings? TODO
\end{itemize}

\section{Gpt-2}

Github user \enquote{nshepperd} created a fork of GPT-2 containing Python scripts to train a GPT-2 model. This code was successfully used by blogger and author \enquote{Gwern} (\url{https://www.gwern.net}) to train a GPT-2 model for creating poetry.


Until the very end it was not possible to determine, how much actual text the model has seen at any training step. 
\begin{itemize}
	\item Welche implementierung und warum?
	\item der blog post mit den gedichten
	\item welche modelle kamen raus
	\item Fragen und Zweifel/Probleme. Undurchsichtige Trainingsskripte usw
	\item sample output und interactive prompt samples
\end{itemize}



\section{What could have gone better? Future Work}

\begin{itemize}
	\item not depend so much on existing repositories/forks 
	\item rather implement own training scripts for gpt2?
	
\end{itemize}


 
%\input{content/introduction}
%\input{content/othercontent}
%\input{content/conclusion}

\printbibliography

\end{document}
